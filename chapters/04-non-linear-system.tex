\chapter{Data-Driven Predictive Method for Non-linear Systems}\label{chap:non-linear-system}

In this chapter, we briefly discuss how to extend the data-driven prediction method to non-linear systems.
To avoid unnecessary complexity, we only consider noise-free systems in this chapter.

First we introduce a method of effectively using one or more dataset trajectories of a non-linear system to make predictions in \cref{sec:weighting-method}.
Then we introduce a metric for evaluating the quality of a dataset in \cref{sec:fractal-dimension-method}.
Finally, we use a numerical example in \cref{sec:non-linear-system-numerical-example} to show that dataset that is better evaluated by the metric can make better predictions using the method we propose.

In this section, we still use $l$ to denote the number of input-output pairs used for implicitly determining the internal state of the system, and use $L$ to denote the prediction horizon.
But in the case of non-linear case, $l$ is just supposed to be long enough to determine the internal state, but lacks a formal definition as in the case of linear systems.


\section{Weighting Method for Non-linear Systems in Data-driven Prediction Methods}\label{sec:weighting-method}

As introduced in \cref{sec:motivation-and-background}, although originally designed for linear systems, experiments show that the data-driven control method can be applied to non-linear systems, such as quadrotors in \cite{elokdaDataQuad2021} and quasi-continuum manipulators in \cite{mullerDataDrivenQCR2022}.

There's also attempt to extend the data-driven control method to general non-linear systems, such as \cite{berberichLinearTrackingMPCData2022}.
Intuitively, this method in \cite{berberichLinearTrackingMPCData2022} is trying to make a linear affine approximation of the non-linear system around current state, by constructing the Hankel matrix using the last $N$ online observations.

But this method has many drawbacks.
It relies on the assumption that online observations are informative enough to approximate the local dynamics of the system, but in practice, the system can fairly be operating consistently in a certain manner, making the online observations not informative enough.
Also, it has very few adjustments that can be done for different systems, but in practice, different systems can behave very differently, and the method should be able to adapt to them.

In this section, we take the intuition of making local linear affine approximation of non-linear systems, and propose a method of making predictions for non-linear systems using dataset trajectories.

As stated in \cref{sec:motivation-and-background}, each column of the Hankel matrix of a long sequence, with depth $L$, is a subsequence of length $L$.
And all columns of the Hankel matrix cover all such subsequences.
Now consider the Hankel matrix of a dataset trajectory $\datasetSequence{u}{N}$ and $\datasetSequence{y}{N}$, with depth $l+L$, which reads:

\begin{equation*}
    \begin{bmatrix}
        \hankel{L+l}{\ud} \\
        \hankel{L+l}{\yd}
    \end{bmatrix} = \begin{bmatrix}
        \ud_0 & \ud_1 & \ldots & \ud_{N-L-l} \\
        \ud_1 & \ud_2 & \ldots & \ud_{N-L-l+1} \\
        \vdots & \vdots & \ddots & \vdots \\
        \ud_{L+l-1} & \ud_{L+l} & \ldots & \ud_{N-1} \\
        \yd_0 & \yd_1 & \ldots & \yd_{N-L-l} \\
        \yd_1 & \yd_2 & \ldots & \yd_{N-L-l+1} \\
        \vdots & \vdots & \ddots & \vdots \\
        \yd_{L+l-1} & \yd_{L+l} & \ldots & \yd_{N-1}
    \end{bmatrix}
\end{equation*}

We can see that each column of the Hankel matrix is a sub-trajectory of length $L+l$.
And all columns of the Hankel matrix cover all such sub-trajectories that can be extracted from the dataset trajectory.
By multiplying the Hankel matrix with a vector $\alpha$, we are essentially taking a linear combination of all such sub-trajectories.
For linear system, such a combination will in turn be a trajectory of the system, and if the dataset trajectory is rich enough (as defined by the persistently exciting of the input sequence), all such linear combinations can cover all possible trajectories of the system.

To simplify further discussion, as in \cite{dorflerBridgingDirectIndirect2023}, we use the following notations:
\begin{align*}
    \Up = \hankel{l}{\subseq{\ud}{0}{N-L-1}} = \begin{bmatrix}
        \ud_0 & \ud_1 & \ldots & \ud_{N-L-l} \\
        \ud_1 & \ud_2 & \ldots & \ud_{N-L-l+1} \\
        \vdots & \vdots & \ddots & \vdots \\
        \ud_{l-1} & \ud_{l} & \ldots & \ud_{N-L-1}
    \end{bmatrix} \\
    \Uf = \hankel{L}{\subseq{\ud}{l}{N-1}} = \begin{bmatrix}
        \ud_l & \ud_{l+1} & \ldots & \ud_{N-L} \\
        \ud_{l+1} & \ud_{l+2} & \ldots & \ud_{N-L+1} \\
        \vdots & \vdots & \ddots & \vdots \\
        \ud_{l+L-1} & \ud_{l+L} & \ldots & \ud_{N-1}
    \end{bmatrix} \\
    \Yp = \hankel{l}{\subseq{\yd}{0}{N-L-1}} = \begin{bmatrix}
        \yd_0 & \yd_1 & \ldots & \yd_{N-L-l} \\
        \yd_1 & \yd_2 & \ldots & \yd_{N-L-l+1} \\
        \vdots & \vdots & \ddots & \vdots \\
        \yd_{l-1} & \yd_{l} & \ldots & \yd_{N-L-1}
    \end{bmatrix} \\
    \Yf = \hankel{L}{\subseq{\yd}{l}{N-1}} = \begin{bmatrix}
        \yd_l & \yd_{l+1} & \ldots & \yd_{N-L} \\
        \yd_{l+1} & \yd_{l+2} & \ldots & \yd_{N-L+1} \\
        \vdots & \vdots & \ddots & \vdots \\
        \yd_{l+L-1} & \yd_{l+L} & \ldots & \yd_{N-1}
    \end{bmatrix}
\end{align*}

That is, separate the first $l$ rows of the Hankel matrix and the last $L$ rows of the Hankel matrix.
Intuitively, the first $l$ rows ($\Up$ and $\Yp$) will be used for fitting the extended initial condition, and the last $L$ rows ($\Uf$ and $\Yf$) will be used for fitting proposed future input sequence and making predictions.

We can also extend this idea to multiple dataset trajectories, as stated in \cite{vanwaardeMultiple2020}.
Suppose we have $K$ dataset trajectories, each of them being $\sequence{\ud^{\scriptscriptstyle{(i)}}}{t}{0}{{N_i}}$, $\sequence{\yd^{\scriptscriptstyle{(i)}}}{t}{0}{{N_i}}$, $i = 1, \ldots, K$, then we can construct the mosaic-Hankel matrix, as defined in \cite{vanwaardeMultiple2020}, as:

\begin{equation*}
    \begin{bmatrix}
        \hankel{L+l}{\ud^{\scriptscriptstyle{(1)}}} & \hankel{L+l}{\ud^{\scriptscriptstyle{(2)}}} & \ldots & \hankel{L+l}{\ud^{\scriptscriptstyle{(K)}}} \\
        \hankel{L+l}{\yd^{\scriptscriptstyle{(1)}}} & \hankel{L+l}{\yd^{\scriptscriptstyle{(2)}}} & \ldots & \hankel{L+l}{\yd^{\scriptscriptstyle{(K)}}} \\
    \end{bmatrix}
\end{equation*}

This mosaic-Hankel matrix contains all possible sub-trajectories of length $L+l$ that can be extracted from the $K$ dataset trajectories.
And we can also separate the first $l$ rows and the last $L$ rows of the mosaic-Hankel matrix, as we do for the singe dataset trajectory case.
We also denote the width of the mosaic-Hankel matrix (number of sub-trajectories) as $\NH$.

Now we can formulate the problem of making predictions for non-linear systems using dataset trajectories as:
\emph{Given $K$ dataset trajectories $\sequence{\ud^{\scriptscriptstyle{(i)}}}{t}{0}{{N_i}}$, $\sequence{\yd^{\scriptscriptstyle{(i)}}}{t}{0}{{N_i}}$, $i = 1, \ldots, K$, the extended initial condition $\xi_t = \begin{bmatrix}\subseq{u}{t-l}{t-1} \\ \subseq{y}{t-l}{t-1}\end{bmatrix}$, and a sequence of future inputs $\subseq{u}{t}{t+L-1}$ How to make good prediction of future outputs $\subseq{y}{t}{t+L-1}$ of the system?}

For now, we suppose that we have a \emph{rich enough} dataset that in principle can cover all possible predictions we want to make about the system, and the only question is how to make good use of the dataset to make predictions.
In \cref{sec:fractal-dimension-method}, we will discuss a metric for evaluating the richness of a dataset.

Taking the intuition from the case of linear systems, we can try to find a good vector $\alpha$, which is used for taking linear combinations of all sub-trajectories of length $L+l$ that can be extracted from the dataset trajectories, to approximate the trajectory $\subseq{u}{t-l}{t+L-1}$ and $\subseq{y}{t-l}{t+L-1}$.

As stated in \cite{dorflerBridgingDirectIndirect2023}, in the LTI system case the problem is formulated as:

\begin{subequations}
\label{eq:original-indirect-method}
\begin{align}
    \min_{\alpha} \quad & \norm{\alpha} \label{eq:original-indirect-method-cost} \\
    \text{s.t.} \quad &
    \begin{bmatrix}
        \Up \\
        \Uf \\
        \Yp
    \end{bmatrix} \alpha = \begin{bmatrix}
        \subseq{u}{t-l}{t-1} \\
        \subseq{u}{t}{T+L-1} \\
        \subseq{y}{t-l}{t-1}
    \end{bmatrix} \label{eq:original-indirect-method-constraint} \\
    & \subseq{y}{t}{t+L-1} = \Yf \alpha \label{eq:original-indirect-method-prediction}
\end{align}
\end{subequations}

The cost \cref{eq:original-indirect-method-cost} is used to make the vector $\alpha$ as small as possible, to deal with possible noise in the dataset.
The constraint \cref{eq:original-indirect-method-constraint} is used to make sure that the extended initial condition $\xi_t$ and proposed future input sequence $\subseq{u}{t}{t+L-1}$ are well fitted by the vector $\alpha$.
And if the system is LTI, and the dataset is rich enough as defined in \cite{vanwaardeMultiple2020}, there will always exist a vector $\alpha$ that satisfies the constraint \cref{eq:original-indirect-method-constraint}.
And the prediction is made by \cref{eq:original-indirect-method-prediction}.

However, in the non-linear case, we need to approximate the system with a \emph{local linear affine} system, which requires the answer to two questions:

\begin{enumerate}
    \item How to find a better way of choosing $\alpha$, so that it can pick sub-trajectories (columns) from the mosaic-Hankel matrix, that are more relevant to the \emph{local} dynamics of the system, near current extended initial condition $\xi_t$?
    \item How to make the prediction based on an \emph{LTI affine system}, not an LTI system?
\end{enumerate}

Note that in non-linear case, the matrix $\begin{bmatrix}
    \Up \\
    \Uf \\
    \Yp
\end{bmatrix}$ is almost always of full row rank, so the constraint \cref{eq:original-indirect-method-constraint} can always be satisfied by some $\alpha$.

For the second question, we can use the result from \cite{martinelliDataDrivenAffine2022}, which states that we just need to add an extra constraint to the optimization problem: $\transpose{\ones{\NH}} \alpha = 1$.

For the first question, we propose a method of weighting different sub-trajectories in the mosaic-Hankel matrix, to put more weight onto sub-trajectories that are closer to the current extended initial condition $\xi_t$, in order to make better approximation of the local dynamics of the system.

For the i-th sub-trajectory (column) of the mosaic-Hankel matrix $\subseq{\ud}{i}{i+L+l-1}$ and $\subseq{\yd}{i}{i+L+l-1}$, we can define a distance to the current extended initial condition $\xi_t$ as: $d\left(\mnormsq{\left(\xi_t - \xi_i\right)}{Q}\right)$, where $\xi_i = \begin{bmatrix}\subseq{u}{i}{i+l-1} \\ \subseq{y}{i}{i+l-1}\end{bmatrix}$ is the extended initial condition of the sub-trajectory (the i-th column of $\Up$ and $\Yp$); $Q$ is a positive definite matrix used to weight the distance; $d$ is a non-decreasing function.

Then, with this distance defined for each sub-trajectory, we can define a diagonal distance matrix $\Dt$ as: $\Dt(i,i) = d\left(\mnormsq{\left(\xi_t - \xi_i\right)}{Q}\right)$, and use this matrix to weight the sub-trajectories in the mosaic-Hankel matrix.
The subscript $t$ is used to emphasize its dependence on the current extended initial condition $\xi_t$.

Then we formulate the problem as:

\begin{subequations}
\label{eq:weighted-indirect-method}
\begin{align}
    \min_{\alpha} \quad & \norm{\alpha}_\Dt \label{eq:weighted-indirect-method-cost} \\
    \text{s.t.} \quad &
    \begin{bmatrix}
        \Up \\
        \Uf \\
        \Yp
    \end{bmatrix} \alpha = \begin{bmatrix}
        \subseq{u}{t-l}{t-1} \\
        \subseq{u}{t}{T+L-1} \\
        \subseq{y}{t-l}{t-1}
    \end{bmatrix} \label{eq:weighted-indirect-method-hankel} \\
    & \transpose{\ones{\NH}} \alpha = 1 \label{eq:weighted-indirect-method-affine} \\
    & \subseq{y}{t}{t+L-1} = \Yf \alpha \label{eq:weighted-indirect-method-prediction}
\end{align}
\end{subequations}

The cost \cref{eq:weighted-indirect-method-cost} is designed to force the choice of $\alpha$ wisely, so that we put more weight to sub-trajectories that are close to the current extended initial condition $\xi_t$.

This optimization problem \cref{eq:weighted-indirect-method} can be solved in closed form, with the optimal $\alpha$ being:

\begin{equation}\label{eq:weighted-indirect-method-solution}
    \alpha = \inv{\Dt}\transpose{\hankeluyone}\inv{\left(\hankeluyone \inv{\Dt} \transpose{\hankeluyone}\right)} \begin{bmatrix}
        \subseq{u}{t-l}{t-1} \\
        \subseq{u}{t}{t+L-1} \\
        \subseq{y}{t-l}{t-1} \\
        1
    \end{bmatrix}
\end{equation}

\begin{remark}\label{remark:weiting-proposed-input}
    It might be tempting to also include the proposed input sequence $\subseq{u}{t}{t+L-1}$ into the weighting of sub-trajectories, and indeed it can be a good idea for the problem of making predictions only.

    However, since the prediction will be used for a predictive controller or safety filter, and the proposed input sequence $\subseq{u}{t}{t+L-1}$ will in these cases be formulated as optimization variables, including them into the weighting method will make the optimization problem non-convex, and thus hard to solve.

    In contrast, the extended initial condition $\xi_t$ is usually given as parameters to the optimization problem, and thus can be used for the weighting method without making the optimization problem non-convex.
\end{remark}

We can see that the solution only depends on the inverse of the distance matrix $\Dt$.
So we can further define a diagonal weighting matrix $\Wt$, where: $\Wt(i,i) = w\left(\mnormsq{\left(\xi_i - \xi_t\right)}{Q}\right)$, where $w$ is a non-increasing function.
And replace $\inv{\Dt}$ by $\Wt$ in the solution \cref{eq:weighted-indirect-method-solution}.

Intuitively, $\Wt(i,i)$ represents the weight of the i-th sub-trajectory, and the weight is higher if the sub-trajectory is closer to the current extended initial condition $\xi_t$.
This method has the advantage that, we can set the weight of sub-trajectories that are far away from the current extended initial condition $\xi_t$ to zero, that is let $\Wt(i,i)=0$, so that they will not be used for making predictions.
This is roughly equivalent to setting $\Dt(i,i)=\infty$, but avoids the problem of dealing with $\infty$.

Then the solution can be rewritten as:

\begin{subequations}
\label{eq:weighted-indirect-method-w-solution}
\begin{align}
    \alpha &= \Wt \transpose{\hankeluyone}\inv{\left(\hankeluyone \Wt \transpose{\hankeluyone}\right)} \begin{bmatrix}
        \subseq{u}{t-l}{t-1} \\
        \subseq{u}{t}{T+L-1} \\
        \subseq{y}{t-l}{t-1} \\
        1
    \end{bmatrix} \label{eq:weighted-indirect-method-w-solution-alpha} \\
    \subseq{y}{t}{t+L-1} &= \Yf \alpha \label{eq:weighted-indirect-method-w-solution-prediction}
\end{align}
\end{subequations}

Lots of possibilities are open for this weighting method.
We just enumerate some of them here.

\begin{enumerate}
    \item The weighting matrix $Q$ and weighting function $w$ can be chosen for different extended initial state $\xi_t$.
    \item There might be a better way to choose these hyperparameters $Q$ and $w$.
    One possible method is to formulate the problem as an optimization problem, with prediction error as the cost.
    In this way it will be possible to use machine learning methods, such as neural networks, to learn the optimal weighting matrix $Q$ and approximate the weighting function $w$.
    \item It is also possible to incorporate the weighting method into the direct method, by using the matrix $\Dt$ to regularize the variable $\alpha$ in formulations like \cref{eq:robust-ddsf-direct}.
\end{enumerate}

These possibilities are left for future work.


\section{Fractal Dimension Method Evaluating Dataset}\label{sec:fractal-dimension-method}

In this section, we introduce a metric for evaluating the richness of a dataset for non-linear system, where the dataset consists of several dataset trajectories as introduced in \cref{sec:weighting-method}.

The intuition is, for a dataset to be rich enough, it should be able to cover all possible input-output pairs of the system.
In another word, we want to evaluate the ability of some curves to cover a certain volume in the higher dimensional space.
To evaluate this \emph{space-filling property}, we can refer to the idea of fractal dimension and space-filling curves, as introduced in \cite{saganSpaceFillingCurves1994} and the box-counting dimension introduced in \cite{kennethAlternativeDefinitionsDimension2003}.
% Here we give a brief introduction to the box-counting dimension.

% \subsection{Introduction to Box-counting Dimension}\label{subsec:box-counting-dimension}

% The box-counting dimension is a method of estimating the fractal dimension of a set $S$.
% For simplicity, we assume that $S$ is a subset of $\Real^d$.
For fractals, the box-counting dimension is defined as:

\begin{equation}
    \label{eq:box-counting-dimension-limit}
    d \defeq \lim_{\epsilon \to 0} \frac{\log N(\epsilon)}{\log \frac{1}{\epsilon}}
\end{equation}

where $N(\epsilon)$ is the number of boxes of size $\epsilon$ needed to cover the fractal.

The intuition is, if $\epsilon$ is small enough, the number of boxes of size $\epsilon$ to cover a smooth manifold of dimension $d$ will be $N(\epsilon) \approx C \left(\frac{1}{\epsilon}\right)^d$, so taking the logarithm of both sides will yield:

\begin{equation}
    \label{eq:box-counting-dimension-log}
    \log \left(N(\epsilon)\right) \approx \log C + d \log \left(\frac{1}{\epsilon}\right)
\end{equation}

and dividing by $\log \frac{1}{\epsilon}$ will give $d$, as $\epsilon \to 0$.
So we use a similar method to determine the dimension of a fractal.

Intuitively, the box-counting dimension can be seen as a metric of how well a set can cover the ambient space.
So we can borrow this idea to evaluate how well the dataset trajectories cover the input-output space $\Yset \times \Uset$.

In practice, we only have access to a finite number of dataset input-output points $\ud_i$ and $\yd_i$, $i = 1, \ldots, N_1+N_2+\dots+N_K$, each of them belonging to a certain dataset trajectory.
We can also try to sample a number of different box sizes $\epsilon$, and count the number of boxes needed to cover all the input-ouput points.

Also, we face the choice of evaluating the box-counting dimension only using input-output pairs within the constraint set $\Yset \times \Uset$ or using the whole dataset.
The former choice might be able to pick the most important part of the dataset, while the latter can give a more comprehensive evaluation of the dataset.
These two choices are also compared in the numerical example in \cref{sec:non-linear-system-numerical-example}.

For illustration, we choose a dataset from the numerical example in \cref{sec:non-linear-system-numerical-example}, which consists of 40 dataset trajectories, each of them having 50 input-output pairs.
As mentioned before, we take one pair of input-output as one point in the input-output space $\Yset \times \Uset$.
In this case, the input-output space is three-dimensional: $\Real^1 \times \Real^2$.
For visualization, we only plot the output part of the trajectory, as seen in \cref{fig:two-dim-dataset-example}.
The preprocessing of the dataset can be found in \cref{sec:non-linear-system-numerical-example}.
In \cref{fig:two-dim-dataset-example}, the x-axis is the first system state $\xelone$, and the y-axis is the second system state $\xeltwo$.
The blue curves represent the dataset trajectories.

\includefig{two-dim-dataset-example.png}{0.6}{Output part of example dataset trajectories}{fig:two-dim-dataset-example}

Then we sample a number of box sizes $\epsilon$, and count the number of boxes needed to cover all the input-output points.
As we care about the logarithm of box size, we sample in the logarithmic space.
In this test case, we choose 30 different $\log_2(\epsilon)$ values with uniform distance from the range $[-5, 0]$.
Note that as the box size becomes smaller, only the sampled box size that results in a different number of box than the previous size is kept.
Then we can plot the number of boxes needed to cover the dataset trajectories, as a function of the logarithm of box size, as seen in \cref{fig:two-dim-dataset-large-size-range}.

\includefig{two-dim-dataset-large-size-range.png}{0.6}{Number of boxes needed to cover the dataset trajectories and box size, drawn in logarithmic scale}{fig:two-dim-dataset-large-size-range}

In the figure, x-axis is the logarithm of reciprocal of box size, and y-axis is the logarithm of the number of boxes needed to cover the dataset trajectories.
The blue dots represent the sampled points.

We can see that, when the box size is not small enough, in \cref{fig:two-dim-dataset-large-size-range} when the size is larger than $2^{-2}$, we can not observe the linear behavior as described in \cref{eq:box-counting-dimension-log}.
This is plausible, as \cref{eq:box-counting-dimension-log} is only a good approximation if the box size is small, even in the case of smooth manifold or actual fractals.

And when the box size is too small, in \cref{fig:two-dim-dataset-large-size-range} when the size is smaller than $2^{-4}$, the number of boxes needed to cover the dataset tends to a constant.
This is due to the fact that we are working with a finite number of dataset points, so when the box size is small enough, each box will contain at most one dataset point, and the number of boxes needed to cover the dataset will be the same as the number of dataset points.

So with this observation, we need to choose a proper range of box size to evaluate the box-counting dimension of the dataset.
As shown in \cref{fig:two-dim-dataset-large-size-range}, we choose the range $[2^{-3.3}, 2^{-2.2}]$ to evaluate the box-counting dimension of the dataset.
Within this region, $\log_2\left(N(\epsilon)\right)$ and $\log_2\left(\frac{1}{\epsilon}\right)$ shows a fairly good linear behavior, as seen in \cref{fig:two-dim-dataset-good-size-range}.

\includefig{two-dim-dataset-good-size-range.png}{0.7}{Number of boxes needed to cover the dataset trajectories and box size, drawn in logarithmic scale, with a proper choice of box size region}{fig:two-dim-dataset-good-size-range}

Then we can fit a linear function to the sampled points in the range $[2^{-4}, 2^{-2}]$, and the slope of the linear function will be the box-counting dimension of the dataset.
In this case, the slope is $2.24$, so the box-counting dimension of the dataset is $2.24$.

The drawbacks and further improvements for this method may include:

\begin{enumerate}
    \item For real datasets, the fractal dimension value heavily depends on the region of box size we choose to evaluate.
    This difference can be larger than the difference between different datasets, as is shown in \cref{sec:non-linear-system-numerical-example}.
    Also, the best region for calculating the fractal dimension may differ for different datasets, so it is unfair to choose a singe region for all datasets.
    Although a method of comparing datasets independent of this region is proposed in \cref{sec:non-linear-system-numerical-example}, a systematic way of choosing this region and getting a single value to compare different datasets is still missing.
    \item The fractal dimension only deals with the space-filling property of a set, but not the size of the covered region.
    For example, if we rescale a fractal into a smaller size, the fractal dimension will not change, but the region it covers will be smaller.
    That also holds for the box-counting dimension of dataset trajectories.
    With a set of dataset trajectories that covers a small region very well but does not contain any information outside the small region, the box-counting dimension might be larger, but the dataset is not so rich.
    It might be possible to include the constant $\log D$ in \cref{eq:box-counting-dimension-log} into consideration, but it is not clear how to do it.
    \item The box-counting dimension can only be a metric for evaluating the richness of a dataset, but does not provide any information about how well the dataset can be used for making predictions or fitting a model.
    The quantitative relation between the box-counting dimension and the quality of prediction or fitted model is still unknown.
\end{enumerate}

These points are left for future investigation.


\section{Numerical Example}\label{sec:non-linear-system-numerical-example}

In this section, we present a numerical example of using the weighting method in \cref{sec:weighting-method} to make predictions for a non-linear system, and using the fractal dimension method in \cref{sec:fractal-dimension-method} to evaluate the richness of the dataset.
As can be seen from the example, the dataset that is better evaluated by the fractal dimension method can make better predictions using the weighting method, with a proper choice of hyperparameters.

For this section, we use the following continuous non-linear system:

\begin{align} \label{eq:continuous-non-linear-system}
    x = \begin{bmatrix}
        \xelone \\
        \xeltwo
    \end{bmatrix} \quad &
    \dot{x} = \begin{bmatrix}
        \dot{\xelone} \\
        \dot{\xeltwo}
    \end{bmatrix} = \begin{bmatrix}
        3 \xeltwo \\
        u - 0.5 \sin(\xelone) - 0.5 \xelone \left(\cos(\xeltwo^2)\right)^2
    \end{bmatrix}
\end{align}

We use a numerical integrator to simulate the system for a sample time $\SI{0.03}{\second}$.
Within each time step, the control input $u$ is kept constant.
We assume the system state is observable.
The system is equipped with state/output constraint $\Yset = \Xset = \left\{x \;|\; -\pi \leq \xelone \leq \pi,\, -0.5 \leq \xeltwo \leq 0.5 \right\}$, and input constraint $\Uset = \left\{u \;|\; -1 \leq u \leq 1 \right\}$.

So we have a discrete time invariant non-linear system $\left(\Xset,\Uset,\Yset,f,g\right)$, where $\Xset$, $\Uset$, $\Yset$ are defined above, $f$ being the map from current state and constant control input of system \cref{eq:continuous-non-linear-system} to next state after $\SI{0.03}{\second}$, and $g\left(x, u\right) = x$.

Since we assume the system state is observable, we can directly deal with the system state without worrying about the output.
The prediction method can still be applied, with the initial input part of mosaic-Hankel matrix $\Up$ being empty and the future input part $\Uf$ being changed to $\Uf = \hankel{L}{\subseq{\ud}{l-1}{N-2}}$.
And \cref{eq:weighted-indirect-method-w-solution} will be reformulated as:

\begin{subequations}
    \label{eq:weighted-indirect-method-w-state-solution}
    \begin{align}
        \alpha &= \Wt \transpose{\hankeluxone}\inv{\left(\hankeluxone \Wt \transpose{\hankeluxone}\right)} \begin{bmatrix}
            \subseq{u}{t}{T+L-1} \\
            x_{t} \\
            1
        \end{bmatrix} \label{eq:weighted-indirect-method-w-satte-solution-alpha} \\
        \subseq{x}{t+1}{t+L} &= \Xf \alpha \label{eq:weighted-indirect-method-w-state-solution-prediction}
\end{align}
\end{subequations}

where $\Xp = \hankel{1}{\subseq{x^d}{0}{N-L-1}}$ and $\Xf = \hankel{L}{\subseq{x^d}{1}{N-1}}$, being the initial state part and future state part of the dataset system state mosaic-Hankel matrix, respectively.

As a new notation, we use variables with check $\check{(-)}$ to denote the real system states or outputs, if the proposed input sequence $\subseq{u}{t}{t+L-1}$ is applied to the real system.
For example, in this specific case, if we further simulate the system for $L$ steps applying $\subseq{u}{t}{t+L-1}$, we will get the state/output sequence $\checkx_j$ and $\checky_j$, $t+1 \leq j \leq t+L$.

For the hyperparameters of the weighting method, we choose $Q = \begin{bmatrix} \frac{1}{4 \pi^2} & 0 \\ 0 & 1 \end{bmatrix}$, to compensate for different size of $\xelone$ and $\xeltwo$; and $w(x) = e^{-\sqrt{x}}$.
Also, we set the weight of $\Wt(i,i)$ to zero if there are enough sub-trajectories that are close enough to current initial condition $x_t$.
The prediction horizon is set to $L = 3$.

For the datasets to be compared, we grid the state space with 10 points in the first dimension and 4 points in the second dimension, and pick the resulting 40 points as the initial conditions.
For each initial condition, we simulate the system for 300 steps with each input uniformly sampled from $\Uset$.
So we have 40 dataset trajectories, each of them having 300 input-output pairs.

This forms the largest dataset, which we call $\dataset_6$.
We also form smaller datasets $\dataset_1$ to $\dataset_5$ by picking the first 50, 100, 150, 200, 250 input-output pairs from each dataset trajectory, respectively.

To evaluate the quality of prediction yield by different datasets, we define the following cost for a given dataset $\dataset$, tested on a certain initial condition $y_t$ and proposed input sequence $\subseq{u}{t}{t+L-1}$:

\begin{equation}\label{eq:prediction-cost-single-point}
    cost\left(\dataset; x_t, \subseq{u}{t}{t+L-1}\right) = \sum_{j=t+1}^{t+L} \mnormsq{\left(\checkx_j - x_j\right)}{Q}
\end{equation}

where $Q$ is the same weighting matrix as used in the weighting method, $y_j$ and $\checky_j$ are the predicted and real output at time step $j$, respectively.

To evaluate the quality of prediction at different initial conditions and input sequences, we also grid the state and input space $\Xset \times \Uset$ with 10 points on each dimension, resulting in 1000 different initial condition and input pairs.
For each pair $x_{t,i}$ and $u_{t,i}$, we start from $x_{t,i}$ and repeat $u_{t,i}$ $L$ times as the proposed input sequence $\subseq{u}{t}{t+L-1}$, then calculate the cost \cref{eq:prediction-cost-single-point} for each dataset $\dataset_1$ to $\dataset_5$.
Then for each dataset, we have 1000 cost values, and we can calculate the mean and maximum of them.

Before calculating the box-counting dimension of the datasets, we normalize the datasets with the bound of the constraint set $\Xset \times \Uset$.

Also, we present two box-dimension values for each dataset, one evaluated using the whole dataset, and one evaluated using only the input-output pairs within the constraint set $\Xset \times \Uset$.

TODO: present the result of error and single box-counting dimension and discussion

{\renewcommand{\arraystretch}{1.3}%
\begin{center}
\captionof{table}{Mean, Maximum Cost and Box-counting Dimension \label{tab:cost-and-dim}}
\begin{tabular}{ >{\raggedleft}p{0.3\linewidth}|c|c|c|c|c|c }
    Dataset & $\dataset_1$ & $\dataset_2$ & $\dataset_3$ & $\dataset_4$ & $\dataset_5$ & $\dataset_6$ \\
    \hline
    Length of single trajectory in dataset & 50 & 100 & 150 & 200 & 250 & 300 \\
    \hline
    Mean cost (\num{e-4})& \num{316.85} & \num{9.49} & \num{5.82} & \num{5.62} & \num{5.29} & \num{5.05} \\
    \hline
    Maximum cost (\num{e-2})& \num{762.84} & \num{2.81} & \num{1.22} & \num{1.20} & \num{1.22} & \num{1.10} \\
\end{tabular}
\end{center}
}

As mentioned in \cref{sec:fractal-dimension-method}, the box-counting dimension of a dataset heavily depends on the region of box size we choose to evaluate.
To overcome this drawback, we propose a method of comparing datasets without choosing a specific box size region for each of them.

The idea is, we can choose a proper range of box size, and grid it into several points.
Then we take each point and calculate the box-counting dimension of the dataset using a size range around that point.
From this process we can get a vector of box-counting dimension values for a dataset, each of them relating to a box size range centered around a certain box size.
By this method we can approximately get the relationship between the box-counting dimension and the box size region, and compare different datasets using this relationship.

We still use the logarithmic scale.
For the specific example, when evaluating the complete dataset, we choose the box size range $[2^{-0}, 2^{-5.0}]$, and grid the range into 50 different points.
For each point, for example $s = -3.0$, we take the size range $[2^{-3.0-0.5}, 2^{-3.0+0.5}]$ to evaluate the box-counting dimension of the dataset.
Then we have \cref{fig:two-dim-example-varying-range-all}, showing the box-counting dimension of six datasets with different range center point.

\includefig{two-dim-example-varying-range-all.png}{0.7}{The relationship between center box-size range (in logarithmic scale) and fitted box-counting dimension of complete dataset}{fig:two-dim-example-varying-range-all}

As we can see from \cref{fig:two-dim-example-varying-range-all}, the box-counting dimension varies a lot with the center point of the box size range, and this variance can be larger than the difference between different datasets.
However, \cref{fig:two-dim-example-varying-range-all} can also be used to compare different datasets independent of the box size range.
We can see that in general, the box-counting dimension of the dataset increases as the length of single dataset trajectory increases.
This can be observed from the fact, for example, the curve corresponds to $\dataset_3$, which is of length 150, is almost always below the curve corresponds to $\dataset_4$, which is of length 200.
In this specific case, the only outlier is the curve corresponds to $\dataset_2$, which is of length 100, but the corresponding curve lies under that of $\dataset_1$, which is of length 50.

We also present \cref{fig:two-dim-example-varying-range-constrained}, which shows the box-counting dimension of six datasets evaluated using only the input-output pairs within the constraint set $\Xset \times \Uset$.

\includefig{two-dim-example-varying-range-constrained.png}{0.7}{The relationship between center box-size range (in logarithmic scale) and fitted box-counting dimension of dataset within cnostraint}{fig:two-dim-example-varying-range-constrained}

In this case, it is clearer that the box-counting dimension of the dataset increases as the length of single dataset trajectory increases, as the curves are almost ordered by the length of single dataset trajectory.

From this example, especially from \cref{tab:cost-and-dim} and \cref{fig:two-dim-example-varying-range-constrained}, we can see that the with properly chosen hyperparameters, the prediction method proposed in \cref{sec:weighting-method} can make better predictions with a dataset that is better evaluated by the fractal dimension method proposed in \cref{sec:fractal-dimension-method}.
